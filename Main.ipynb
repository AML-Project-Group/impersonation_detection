{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' PRE-PROCESSING (INDIVIDUAL 1) '''\n",
    "''' Michelangelo Rubino '''\n",
    "\n",
    "# The function preproc_df(filename) at the end of this stage can be used to perform the following transformations:\n",
    "\n",
    "# Step 1 - delete the features with no values\n",
    "# Step 2 - delete the features with duplicated values \n",
    "# Step 3 - remove highly correlated features (kendall/spearman >= 0.9)\n",
    "# Step 4 - standardize the features to give them a distribution closer to the normal one\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# uncomment to show every output rows\n",
    "#pd.set_option('display.max_rows', 200) \n",
    "\n",
    "# uncomment to show every output columns\n",
    "#pd.set_option('display.max_columns', 200) \n",
    "\n",
    "set_printoptions(precision=3) # how the floating numbers are shown\n",
    "\n",
    "''' Load the data from the train dataset and create a dataframe '''\n",
    "\n",
    "filename = 'train_imperson_without4n7_balanced_data.csv'\n",
    "\n",
    "# use the code below to load the test dataset\n",
    "# filename = 'test_imperson_without4n7_balanced_data.csv'\n",
    "\n",
    "df = pd.read_csv(filename,\n",
    "                header=0,\n",
    "                na_values=['nan'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97044 entries, 0 to 97043\n",
      "Columns: 153 entries, 1 to 155\n",
      "dtypes: float64(48), int64(105)\n",
      "memory usage: 113.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "''' View dataframe info '''\n",
    "\n",
    "# the dataframe does not contain null values (it can be verified by using df.isnull().sum())\n",
    "# 97,044 rows, 153 columns\n",
    "# the class is the feature 155\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "0    48522\n",
      "1    48522\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "''' The observations are perfectly balanced: 50% class 0, 50% class 1 '''\n",
    "class_counts = df.groupby('155').size()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035528</td>\n",
       "      <td>0.035528</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.094771</td>\n",
       "      <td>0.094771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035116</td>\n",
       "      <td>0.035116</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.094771</td>\n",
       "      <td>0.094771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>0.073203</td>\n",
       "      <td>0.073203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.144440</td>\n",
       "      <td>0.144440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.094771</td>\n",
       "      <td>0.094771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3         5         6         8         9  10  11  12  ...  146  147  \\\n",
       "0  0  0  0  0.000066  0.000066  0.009150  0.009150   0   0   0  ...  0.0    0   \n",
       "1  0  0  0  0.000014  0.000014  0.000000  0.000000   0   0   0  ...  0.0    0   \n",
       "2  0  0  0  0.035528  0.035528  0.070588  0.070588   0   0   0  ...  0.0    0   \n",
       "3  0  0  0  0.005128  0.005128  0.094771  0.094771   0   0   0  ...  0.0    0   \n",
       "4  0  0  0  0.035116  0.035116  0.070588  0.070588   0   0   0  ...  0.0    0   \n",
       "5  0  0  0  0.005099  0.005099  0.094771  0.094771   0   0   0  ...  0.0    0   \n",
       "6  0  0  0  0.020133  0.020133  0.073203  0.073203   0   0   0  ...  0.0    0   \n",
       "7  0  0  0  0.140800  0.140800  0.144440  0.144440   0   0   0  ...  0.0    0   \n",
       "8  0  0  0  0.004916  0.004916  0.094771  0.094771   0   0   0  ...  0.0    0   \n",
       "9  0  0  0  0.034137  0.034137  0.070588  0.070588   0   0   0  ...  0.0    0   \n",
       "\n",
       "   148  149  150  151  152  153  154  155  \n",
       "0    0    0    0    0    0    0  0.0    0  \n",
       "1    0    0    0    0    0    0  0.0    0  \n",
       "2    0    0    0    0    0    0  0.0    0  \n",
       "3    0    0    0    0    0    0  0.0    0  \n",
       "4    0    0    0    0    0    0  0.0    0  \n",
       "5    0    0    0    0    0    0  0.0    0  \n",
       "6    0    0    0    0    0    0  0.0    0  \n",
       "7    0    0    0    0    0    0  0.0    0  \n",
       "8    0    0    0    0    0    0  0.0    0  \n",
       "9    0    0    0    0    0    0  0.0    0  \n",
       "\n",
       "[10 rows x 153 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Check the first 10 rows '''\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1        2        3             5             6             8  \\\n",
      "count  97044.0  97044.0  97044.0  97044.000000  97044.000000  97044.000000   \n",
      "mean       0.0      0.0      0.0      0.006252      0.006252      0.193837   \n",
      "std        0.0      0.0      0.0      0.015541      0.015541      0.354444   \n",
      "min        0.0      0.0      0.0      0.000003      0.000003      0.000000   \n",
      "25%        0.0      0.0      0.0      0.001442      0.001442      0.037908   \n",
      "50%        0.0      0.0      0.0      0.003706      0.003706      0.037908   \n",
      "75%        0.0      0.0      0.0      0.005916      0.005916      0.054902   \n",
      "max        0.0      0.0      0.0      0.978440      0.978440      1.000000   \n",
      "\n",
      "                  9       10       11       12  ...           146      147  \\\n",
      "count  97044.000000  97044.0  97044.0  97044.0  ...  97044.000000  97044.0   \n",
      "mean       0.193837      0.0      0.0      0.0  ...      0.028436      0.0   \n",
      "std        0.354444      0.0      0.0      0.0  ...      0.062765      0.0   \n",
      "min        0.000000      0.0      0.0      0.0  ...      0.000000      0.0   \n",
      "25%        0.037908      0.0      0.0      0.0  ...      0.000000      0.0   \n",
      "50%        0.037908      0.0      0.0      0.0  ...      0.000000      0.0   \n",
      "75%        0.054902      0.0      0.0      0.0  ...      0.000000      0.0   \n",
      "max        1.000000      0.0      0.0      0.0  ...      1.000000      0.0   \n",
      "\n",
      "           148      149      150      151      152      153           154  \\\n",
      "count  97044.0  97044.0  97044.0  97044.0  97044.0  97044.0  97044.000000   \n",
      "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.178474   \n",
      "std        0.0      0.0      0.0      0.0      0.0      0.0      0.360078   \n",
      "min        0.0      0.0      0.0      0.0      0.0      0.0      0.000000   \n",
      "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.000000   \n",
      "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.023873   \n",
      "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.023873   \n",
      "max        0.0      0.0      0.0      0.0      0.0      0.0      1.000000   \n",
      "\n",
      "                155  \n",
      "count  97044.000000  \n",
      "mean       0.500000  \n",
      "std        0.500003  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.500000  \n",
      "75%        1.000000  \n",
      "max        1.000000  \n",
      "\n",
      "[8 rows x 153 columns]\n"
     ]
    }
   ],
   "source": [
    "''' Descriptive statistics '''\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '6' '8' '9' '14' '15' '16' '18' '20' '26' '29' '38' '43' '47' '48'\n",
      " '50' '51' '52' '61' '62' '64' '66' '67' '68' '70' '71' '72' '73' '75'\n",
      " '76' '77' '78' '79' '80' '82' '83' '84' '86' '88' '89' '90' '93' '94'\n",
      " '97' '98' '104' '105' '106' '107' '108' '109' '110' '111' '112' '113'\n",
      " '117' '118' '119' '120' '121' '122' '123' '125' '126' '127' '128' '129'\n",
      " '130' '133' '138' '140' '141' '142' '143' '144' '145' '146' '154' '155']\n"
     ]
    }
   ],
   "source": [
    "''' Step 1 - Delete columns with no values '''\n",
    "\n",
    "columnsToDelete = [] \n",
    "\n",
    "# if the feature has std == 0 it means no values or only one value\n",
    "for key, value in df.iteritems():\n",
    "    if df[key].std() == 0:\n",
    "        columnsToDelete.append(key)\n",
    "        \n",
    "df1 = df.drop(columnsToDelete, axis=1) # 74 features have been deleted\n",
    "\n",
    "# the features are now 78 + 1 class\n",
    "print(df1.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '8' '14' '38' '47' '48' '50' '51' '61' '64' '66' '67' '68' '70' '71'\n",
      " '72' '73' '75' '76' '77' '78' '79' '80' '82' '83' '84' '86' '88' '90'\n",
      " '93' '94' '97' '98' '104' '105' '106' '107' '108' '109' '110' '111' '112'\n",
      " '113' '117' '118' '119' '120' '121' '122' '123' '125' '126' '127' '128'\n",
      " '129' '130' '138' '140' '141' '142' '143' '144' '145' '154' '155']\n"
     ]
    }
   ],
   "source": [
    "''' Step 2 - Some features are duplicates of other features, then they can be removed keeping only the first column '''\n",
    "\n",
    "# df has 14 duplicated features, then, after removing them, the new dataframe has 64 features + 1 class\n",
    "df2 = df1.loc[:,~df1.T.duplicated(keep='first')]\n",
    "\n",
    "# the features are now 64 + 1 class\n",
    "print(df2.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5       8718\n",
      "8        399\n",
      "14         2\n",
      "38     25023\n",
      "47        12\n",
      "48        11\n",
      "50         2\n",
      "51         2\n",
      "61        63\n",
      "64        20\n",
      "66         3\n",
      "67        12\n",
      "68         3\n",
      "70         2\n",
      "71         2\n",
      "72         2\n",
      "73         2\n",
      "75        80\n",
      "76       133\n",
      "77        84\n",
      "78        42\n",
      "79        46\n",
      "80        18\n",
      "82      4096\n",
      "83         2\n",
      "84         2\n",
      "86         2\n",
      "88         4\n",
      "90         2\n",
      "93         2\n",
      "       ...  \n",
      "106        2\n",
      "107     1738\n",
      "108        2\n",
      "109        4\n",
      "110        3\n",
      "111        2\n",
      "112        4\n",
      "113        2\n",
      "117        6\n",
      "118        2\n",
      "119       34\n",
      "120       14\n",
      "121        2\n",
      "122        3\n",
      "123        2\n",
      "125        3\n",
      "126        2\n",
      "127        3\n",
      "128        3\n",
      "129        2\n",
      "130        2\n",
      "138        2\n",
      "140    17320\n",
      "141        4\n",
      "142    20359\n",
      "143     1076\n",
      "144       75\n",
      "145        3\n",
      "154      333\n",
      "155        2\n",
      "Length: 65, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "''' Check the unique values for each feature '''\n",
    "\n",
    "print(df2.nunique()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Plot the distribution for the 3 main groups:\n",
    "\n",
    "    - features with continuous values between 0 and 1 (ex: 38)\n",
    "    - features with only two values, 0 and 1 (ex: 71)\n",
    "    - features with a few values (ex: 145) '''\n",
    "\n",
    "# no feature with a perfect normal distribution\n",
    "df2[[\"38\", \"71\", \"145\"]].plot.kde(figsize=(5,3), sharex=False)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['47', '50', '64', '66', '68', '84', '86', '90', '97', '98', '107', '108', '118', '119', '126', '127', '128', '129', '141']\n"
     ]
    }
   ],
   "source": [
    "''' Step 3 - Create two correlation matrices (Kendall and Spearman) and remove highly correlated features (ie. >= 0.9 )'''\n",
    "\n",
    "# Create correlation matrix with kendall method\n",
    "corr_matrix_kendall = df2.corr(method='kendall').abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask_kendall = np.triu(np.ones_like(corr_matrix_kendall, dtype=bool))\n",
    "tri_df_kendall = corr_matrix_kendall.mask(mask_kendall)\n",
    "\n",
    "# List column names of highly correlated features ( >= 0.90 )\n",
    "kendall_to_drop = [c for c in tri_df_kendall.columns if any(tri_df_kendall[c] >= 0.90)]\n",
    "\n",
    "# Create correlation matrix with spearman method\n",
    "corr_matrix_spearman = df2.corr(method='spearman').abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask_spearman = np.triu(np.ones_like(corr_matrix_spearman, dtype=bool))\n",
    "tri_df_spearman = corr_matrix_spearman.mask(mask_spearman)\n",
    "\n",
    "# List column names of highly correlated features ( >= 0.90 )\n",
    "spearman_to_drop = [c for c in tri_df_spearman.columns if any(tri_df_spearman[c] >= 0.90)]\n",
    "\n",
    "# The two methods show almost the same features, 19 are in common, 1 is not\n",
    "drop_high_corr = []\n",
    "for i in kendall_to_drop:\n",
    "    if i in spearman_to_drop:\n",
    "        drop_high_corr.append(i)\n",
    "\n",
    "# drop the 19 common features with high correlation both with kendall and spearman       \n",
    "df3 = df2.drop(drop_high_corr, axis=1)  \n",
    "\n",
    "# print the common features removed from the dataframe\n",
    "print(drop_high_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "['5' '8' '14' '38' '48' '51' '61' '67' '70' '71' '72' '73' '75' '76' '77'\n",
      " '78' '79' '80' '82' '83' '88' '93' '94' '104' '105' '106' '109' '110'\n",
      " '111' '112' '113' '117' '120' '121' '122' '123' '125' '130' '138' '140'\n",
      " '142' '143' '144' '145' '154' '155']\n"
     ]
    }
   ],
   "source": [
    "''' After this step the dataframe has 45 features and 1 class '''\n",
    "\n",
    "print(len(df3.columns.values))\n",
    "print(df3.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Analysis and plotting of the dataframe df3 show that none of the features has a normal form '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Analysis and plotting of the dataframe df3 show that none of the features has a normal form '''\n",
    "\n",
    "# Uncomment to see the plots\n",
    "\n",
    "# skewness\n",
    "#skew = df3.skew()\n",
    "#print(skew)\n",
    "\n",
    "# histogram plot\n",
    "#df3.hist(layout=(9,9), figsize=(30,14))\n",
    "#pyplot.show()\n",
    "\n",
    "# density plot\n",
    "#df3.plot(kind='density', subplots=True, layout=(9,9), sharex=False, figsize=(20,14))\n",
    "#pyplot.show()\n",
    "\n",
    "# Box and Whisker Plot\n",
    "#df3.plot(kind='box', subplots=True, layout=(9,9), sharex=False, sharey=False, figsize=(20,14))\n",
    "#pyplot.show()\n",
    "\n",
    "# Scatterplot Matrix\n",
    "# it takes a lot to run\n",
    "#scatter_matrix(df3, figsize=[5, 5])\n",
    "#pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Step 4 - Standardize the features to give them a normal form: mean = 0, sd = 1 '''\n",
    "\n",
    "# save the features name\n",
    "features_names = df3.columns.values\n",
    "\n",
    "# use the array lenght\n",
    "df3_len = len(df3.columns.values) - 1\n",
    "\n",
    "# extract the values from the dataframe\n",
    "array = df3.values\n",
    "X = array[:, 0:df3_len] # extract the features\n",
    "Y = array[:, df3_len].astype(int) # extract the class\n",
    "\n",
    "# fit and transform the values\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "# a new dataframe is created with the standardized values\n",
    "df4 = pd.DataFrame(rescaledX, columns=features_names[0:df3_len])\n",
    "\n",
    "# add the class 155\n",
    "df4.insert(df3_len, '155', Y, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          5         8        14        38       48        51        61  \\\n",
      "0 -0.398089 -0.521063  0.014712 -3.142811  0.01475  1.534041  0.708991   \n",
      "1 -0.401396 -0.546879  0.014712 -3.142628  0.01475  1.534041  0.708991   \n",
      "2  1.883791 -0.347726  0.014712 -3.142264  0.01475 -0.651873 -0.254672   \n",
      "3 -0.072373 -0.279498  0.014712 -3.141809  0.01475 -0.651873 -4.109153   \n",
      "4  1.857280 -0.347726  0.014712 -3.141445  0.01475 -0.651873 -0.013756   \n",
      "\n",
      "         67        70        71  ...       125       130      138       140  \\\n",
      "0 -0.175252 -0.285459  1.391247  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
      "1  1.575793 -0.285459 -0.718780  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
      "2  0.602982 -0.285459 -0.718780  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
      "3  0.602982 -0.285459 -0.718780  ... -0.038843  6.740366 -0.00321 -1.050047   \n",
      "4  0.602982 -0.285459 -0.718780  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
      "\n",
      "        142       143      144       145       154  155  \n",
      "0 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
      "1 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
      "2 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
      "3 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
      "4 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "''' Now we have a dataframe with standardized values '''\n",
    "\n",
    "print(df4.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' After standardizing, the features do not show a perfect bell-shaped distribution, but there\n",
    "    is some improvement '''\n",
    "\n",
    "df4[[\"38\", \"71\", \"145\"]].plot.kde(figsize=(5,3), sharex=False )\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function performs the steps previously described '''\n",
    "\n",
    "filename = 'train_imperson_without4n7_balanced_data.csv'\n",
    "\n",
    "def preproc_df(filename):\n",
    "    dataframe = pd.read_csv(filename, header=0, na_values=['nan'])\n",
    "    \n",
    "    # Step 1 - remove features with no values\n",
    "    columnsToDelete = [] \n",
    "    for key, value in dataframe.iteritems():\n",
    "        if dataframe[key].std() == 0:\n",
    "            columnsToDelete.append(key)\n",
    "        \n",
    "    df1 = dataframe.drop(columnsToDelete, axis=1)\n",
    "    \n",
    "    # Step 2 - remove duplicated features, keeping only the first one \n",
    "    df2 = df1.loc[:,~df1.T.duplicated(keep='first')]\n",
    "    \n",
    "    # Step 3 - Compute correlation matrices and drop the features with high correlation \n",
    "    corr_matrix_kendall = df2.corr(method='kendall').abs()\n",
    "    mask_kendall = np.triu(np.ones_like(corr_matrix_kendall, dtype=bool))\n",
    "    tri_df_kendall = corr_matrix_kendall.mask(mask_kendall)\n",
    "    kendall_to_drop = [c for c in tri_df_kendall.columns if any(tri_df_kendall[c] >= 0.90)]\n",
    "\n",
    "    corr_matrix_spearman = df2.corr(method='spearman').abs()\n",
    "    mask_spearman = np.triu(np.ones_like(corr_matrix_spearman, dtype=bool))\n",
    "    tri_df_spearman = corr_matrix_spearman.mask(mask_spearman)\n",
    "    spearman_to_drop = [c for c in tri_df_spearman.columns if any(tri_df_spearman[c] >= 0.90)]\n",
    "    \n",
    "    # list the features having high kendall and spearman correlation\n",
    "    drop_high_corr = []\n",
    "    for i in kendall_to_drop:\n",
    "        if i in spearman_to_drop:\n",
    "            drop_high_corr.append(i)\n",
    "\n",
    "    # drop the common features       \n",
    "    df3 = df2.drop(drop_high_corr, axis=1)  \n",
    "    \n",
    "    # Step 4 - Standardize the features\n",
    "    features_names = df3.columns.values # save the feature names\n",
    "    df3_len = len(df3.columns.values) - 1 # use the array length \n",
    "\n",
    "    # extract the values from the dataframe (features and class)\n",
    "    array = df3.values\n",
    "    X = array[:, 0:df3_len] \n",
    "    Y = array[:, df3_len].astype(int) \n",
    "\n",
    "    # fit and transform the values\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    rescaledX = scaler.transform(X)\n",
    "\n",
    "    # a new dataframe is created with the standardized values\n",
    "    df4 = pd.DataFrame(rescaledX, columns=features_names[0:df3_len])\n",
    "\n",
    "    # add the class 155\n",
    "    df4.insert(df3_len, '155', Y, False)\n",
    "\n",
    "    return df4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Before the Feature selection stage the train dataset has 45 features and 1 class\\n\\n['5' '8' '14' '38' '48' '51' '61' '67' '70' '71' '72' '73' '75' '76' '77'\\n '78' '79' '80' '82' '83' '88' '93' '94' '104' '105' '106' '109' '110'\\n '111' '112' '113' '117' '120' '121' '122' '123' '125' '130' '138' '140'\\n '142' '143' '144' '145' '154' '155']\\n\\n\\nwhile the test dataset has 43 features and 1 class\\n\\n['5' '8' '14' '38' '47' '48' '51' '61' '64' '67' '70' '71' '72' '73' '75'\\n '76' '77' '79' '80' '81' '82' '94' '104' '105' '106' '109' '110' '111'\\n '112' '120' '121' '122' '123' '125' '128' '140' '141' '142' '143' '144'\\n '145' '148' '154' '155']\\n \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Before the Feature selection stage the train dataset has 45 features and 1 class\n",
    "\n",
    "['5' '8' '14' '38' '48' '51' '61' '67' '70' '71' '72' '73' '75' '76' '77'\n",
    " '78' '79' '80' '82' '83' '88' '93' '94' '104' '105' '106' '109' '110'\n",
    " '111' '112' '113' '117' '120' '121' '122' '123' '125' '130' '138' '140'\n",
    " '142' '143' '144' '145' '154' '155']\n",
    "\n",
    "\n",
    "while the test dataset has 43 features and 1 class\n",
    "\n",
    "['5' '8' '14' '38' '47' '48' '51' '61' '64' '67' '70' '71' '72' '73' '75'\n",
    " '76' '77' '79' '80' '81' '82' '94' '104' '105' '106' '109' '110' '111'\n",
    " '112' '120' '121' '122' '123' '125' '128' '140' '141' '142' '143' '144'\n",
    " '145' '148' '154' '155']\n",
    " \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FEATURE SELECTION (INDIVIDUAL 2) '''\n",
    "''' Ian Dickerson '''\n",
    "from sklearn.feature_selection import mutual_info_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>14</th>\n",
       "      <th>38</th>\n",
       "      <th>48</th>\n",
       "      <th>51</th>\n",
       "      <th>61</th>\n",
       "      <th>67</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>...</th>\n",
       "      <th>125</th>\n",
       "      <th>130</th>\n",
       "      <th>138</th>\n",
       "      <th>140</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.398089</td>\n",
       "      <td>-0.521063</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.142811</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>1.534041</td>\n",
       "      <td>0.708991</td>\n",
       "      <td>-0.175252</td>\n",
       "      <td>-0.285459</td>\n",
       "      <td>1.391247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.401396</td>\n",
       "      <td>-0.546879</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.142628</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>1.534041</td>\n",
       "      <td>0.708991</td>\n",
       "      <td>1.575793</td>\n",
       "      <td>-0.285459</td>\n",
       "      <td>-0.718780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.883791</td>\n",
       "      <td>-0.347726</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.142264</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>-0.651873</td>\n",
       "      <td>-0.254672</td>\n",
       "      <td>0.602982</td>\n",
       "      <td>-0.285459</td>\n",
       "      <td>-0.718780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.072373</td>\n",
       "      <td>-0.279498</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.141809</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>-0.651873</td>\n",
       "      <td>-4.109153</td>\n",
       "      <td>0.602982</td>\n",
       "      <td>-0.285459</td>\n",
       "      <td>-0.718780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>6.740366</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.857280</td>\n",
       "      <td>-0.347726</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.141445</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>-0.651873</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>0.602982</td>\n",
       "      <td>-0.285459</td>\n",
       "      <td>-0.718780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          5         8        14        38       48        51        61  \\\n",
       "0 -0.398089 -0.521063  0.014712 -3.142811  0.01475  1.534041  0.708991   \n",
       "1 -0.401396 -0.546879  0.014712 -3.142628  0.01475  1.534041  0.708991   \n",
       "2  1.883791 -0.347726  0.014712 -3.142264  0.01475 -0.651873 -0.254672   \n",
       "3 -0.072373 -0.279498  0.014712 -3.141809  0.01475 -0.651873 -4.109153   \n",
       "4  1.857280 -0.347726  0.014712 -3.141445  0.01475 -0.651873 -0.013756   \n",
       "\n",
       "         67        70        71  ...       125       130      138       140  \\\n",
       "0 -0.175252 -0.285459  1.391247  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "1  1.575793 -0.285459 -0.718780  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "2  0.602982 -0.285459 -0.718780  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "3  0.602982 -0.285459 -0.718780  ... -0.038843  6.740366 -0.00321 -1.050047   \n",
       "4  0.602982 -0.285459 -0.718780  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "\n",
       "        142       143      144       145       154  155  \n",
       "0 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "1 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "2 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "3 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "4 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = preproc_df(\"./train_imperson_without4n7_balanced_data.csv\")\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "Build a separate dataframe to track information about the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(index = processed_df.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"MI\"] = mutual_info_classif(processed_df.iloc[:,:-1],processed_df[\"155\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"absrho\"] = [abs(processed_df[f].corr(processed_df[\"155\"],method='pearson' )) for f in features.index]\n",
    "features[\"abstau\"] = [abs(processed_df[f].corr(processed_df[\"155\"],method='kendall' )) for f in features.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the features with high MI, rho or tau can be identified with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MI</th>\n",
       "      <th>absrho</th>\n",
       "      <th>abstau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.445165</td>\n",
       "      <td>0.058410</td>\n",
       "      <td>0.517106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.635773</td>\n",
       "      <td>0.438183</td>\n",
       "      <td>0.119957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.648770</td>\n",
       "      <td>0.496848</td>\n",
       "      <td>0.448580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.280243</td>\n",
       "      <td>0.651828</td>\n",
       "      <td>0.651828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.476383</td>\n",
       "      <td>0.838937</td>\n",
       "      <td>0.783199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.318148</td>\n",
       "      <td>0.708561</td>\n",
       "      <td>0.708561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.129447</td>\n",
       "      <td>0.477183</td>\n",
       "      <td>0.477183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.276124</td>\n",
       "      <td>0.296245</td>\n",
       "      <td>0.620754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.478308</td>\n",
       "      <td>0.099870</td>\n",
       "      <td>0.747373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.487864</td>\n",
       "      <td>0.201783</td>\n",
       "      <td>0.289967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.525530</td>\n",
       "      <td>0.081860</td>\n",
       "      <td>0.725743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.151652</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>0.452988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.582116</td>\n",
       "      <td>0.493602</td>\n",
       "      <td>0.234276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.576537</td>\n",
       "      <td>0.180674</td>\n",
       "      <td>0.245990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.575332</td>\n",
       "      <td>0.115751</td>\n",
       "      <td>0.230346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.140461</td>\n",
       "      <td>0.453058</td>\n",
       "      <td>0.453468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.526461</td>\n",
       "      <td>0.434536</td>\n",
       "      <td>0.006752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MI    absrho    abstau\n",
       "5    0.445165  0.058410  0.517106\n",
       "8    0.635773  0.438183  0.119957\n",
       "38   0.648770  0.496848  0.448580\n",
       "51   0.280243  0.651828  0.651828\n",
       "67   0.476383  0.838937  0.783199\n",
       "71   0.318148  0.708561  0.708561\n",
       "73   0.129447  0.477183  0.477183\n",
       "75   0.276124  0.296245  0.620754\n",
       "76   0.478308  0.099870  0.747373\n",
       "77   0.487864  0.201783  0.289967\n",
       "79   0.525530  0.081860  0.725743\n",
       "80   0.151652  0.030155  0.452988\n",
       "82   0.582116  0.493602  0.234276\n",
       "140  0.576537  0.180674  0.245990\n",
       "142  0.575332  0.115751  0.230346\n",
       "145  0.140461  0.453058  0.453468\n",
       "154  0.526461  0.434536  0.006752"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.loc[(features.MI > 0.4)|(features.absrho > 0.4)|(features.abstau > 0.4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pcaX = pca.fit_transform(processed_df.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the PCs to the processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>14</th>\n",
       "      <th>38</th>\n",
       "      <th>48</th>\n",
       "      <th>...</th>\n",
       "      <th>125</th>\n",
       "      <th>130</th>\n",
       "      <th>138</th>\n",
       "      <th>140</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060897</td>\n",
       "      <td>1.251821</td>\n",
       "      <td>0.637731</td>\n",
       "      <td>-0.403237</td>\n",
       "      <td>0.461818</td>\n",
       "      <td>-0.398089</td>\n",
       "      <td>-0.521063</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.142811</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.340807</td>\n",
       "      <td>1.522239</td>\n",
       "      <td>0.880015</td>\n",
       "      <td>-1.062445</td>\n",
       "      <td>0.701038</td>\n",
       "      <td>-0.401396</td>\n",
       "      <td>-0.546879</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.142628</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.966310</td>\n",
       "      <td>3.504782</td>\n",
       "      <td>-4.585714</td>\n",
       "      <td>-0.360268</td>\n",
       "      <td>0.442873</td>\n",
       "      <td>1.883791</td>\n",
       "      <td>-0.347726</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.142264</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.274253</td>\n",
       "      <td>4.534967</td>\n",
       "      <td>-6.029456</td>\n",
       "      <td>0.173649</td>\n",
       "      <td>-1.190417</td>\n",
       "      <td>-0.072373</td>\n",
       "      <td>-0.279498</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.141809</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>6.740366</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.255529</td>\n",
       "      <td>4.520726</td>\n",
       "      <td>-7.143773</td>\n",
       "      <td>-0.375010</td>\n",
       "      <td>0.560924</td>\n",
       "      <td>1.857280</td>\n",
       "      <td>-0.347726</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>-3.141445</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038843</td>\n",
       "      <td>-0.148360</td>\n",
       "      <td>-0.00321</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>-0.02603</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4       PC5         5         8  \\\n",
       "0  0.060897  1.251821  0.637731 -0.403237  0.461818 -0.398089 -0.521063   \n",
       "1  0.340807  1.522239  0.880015 -1.062445  0.701038 -0.401396 -0.546879   \n",
       "2  4.966310  3.504782 -4.585714 -0.360268  0.442873  1.883791 -0.347726   \n",
       "3  7.274253  4.534967 -6.029456  0.173649 -1.190417 -0.072373 -0.279498   \n",
       "4  6.255529  4.520726 -7.143773 -0.375010  0.560924  1.857280 -0.347726   \n",
       "\n",
       "         14        38       48  ...       125       130      138       140  \\\n",
       "0  0.014712 -3.142811  0.01475  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "1  0.014712 -3.142628  0.01475  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "2  0.014712 -3.142264  0.01475  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "3  0.014712 -3.141809  0.01475  ... -0.038843  6.740366 -0.00321 -1.050047   \n",
       "4  0.014712 -3.141445  0.01475  ... -0.038843 -0.148360 -0.00321 -1.050047   \n",
       "\n",
       "        142       143      144       145       154  155  \n",
       "0 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "1 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "2 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "3 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "4 -0.995985 -0.091603 -0.02603 -0.453058 -0.495656    0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_with_pcs = pd.DataFrame(pcaX,columns=[f\"PC{i}\" for i in range(1,6)]).join(processed_df)\n",
    "processed_with_pcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(filename, rho=0.4, tau=0.4, mi=0.4, pcs=5):\n",
    "    \n",
    "    # Preprocessing\n",
    "    processed_data = preproc_df(filename)\n",
    "    # Feature information tracking\n",
    "    features = pd.DataFrame(index = processed_data.columns[:-1])\n",
    "    features[\"MI\"] = mutual_info_classif(processed_data.iloc[:,:-1],processed_data[\"155\"])\n",
    "    features[\"absrho\"] = [abs(processed_data[f].corr(processed_data[\"155\"],method='pearson' )) for f in features.index]\n",
    "    features[\"abstau\"] = [abs(processed_data[f].corr(processed_data[\"155\"],method='kendall' )) for f in features.index]\n",
    "    # Principal component analysis\n",
    "    pca = PCA(n_components=pcs)\n",
    "    pcaX = pca.fit_transform(processed_data.iloc[:,:-1])\n",
    "    pc_names = [f\"PC{i}\" for i in range(1,pcs+1)]\n",
    "    # Select features that meet the thresholds, plus PCs\n",
    "    feature_names = list(features.loc[(features.MI > mi)|(features.absrho > rho)|(features.abstau > tau)].index)\n",
    "    return pd.DataFrame(pcaX,columns=pc_names).join(processed_data[feature_names+[\"155\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>38</th>\n",
       "      <th>51</th>\n",
       "      <th>67</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>82</th>\n",
       "      <th>140</th>\n",
       "      <th>142</th>\n",
       "      <th>145</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060161</td>\n",
       "      <td>1.252939</td>\n",
       "      <td>0.644791</td>\n",
       "      <td>-0.418914</td>\n",
       "      <td>0.443708</td>\n",
       "      <td>-0.398089</td>\n",
       "      <td>-0.521063</td>\n",
       "      <td>-3.142811</td>\n",
       "      <td>1.534041</td>\n",
       "      <td>-0.175252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103169</td>\n",
       "      <td>-0.162696</td>\n",
       "      <td>-0.138420</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>2.653966</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.340383</td>\n",
       "      <td>1.525171</td>\n",
       "      <td>0.900528</td>\n",
       "      <td>-1.114550</td>\n",
       "      <td>0.632546</td>\n",
       "      <td>-0.401396</td>\n",
       "      <td>-0.546879</td>\n",
       "      <td>-3.142628</td>\n",
       "      <td>1.534041</td>\n",
       "      <td>1.575793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049529</td>\n",
       "      <td>-0.320552</td>\n",
       "      <td>-0.218941</td>\n",
       "      <td>-0.226305</td>\n",
       "      <td>-0.831461</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.967133</td>\n",
       "      <td>3.503331</td>\n",
       "      <td>-4.593643</td>\n",
       "      <td>-0.403800</td>\n",
       "      <td>0.503636</td>\n",
       "      <td>1.883791</td>\n",
       "      <td>-0.347726</td>\n",
       "      <td>-3.142264</td>\n",
       "      <td>-0.651873</td>\n",
       "      <td>0.602982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129992</td>\n",
       "      <td>-0.241625</td>\n",
       "      <td>-0.122317</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>0.109363</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.268661</td>\n",
       "      <td>4.545243</td>\n",
       "      <td>-6.048977</td>\n",
       "      <td>0.167516</td>\n",
       "      <td>-1.561113</td>\n",
       "      <td>-0.072373</td>\n",
       "      <td>-0.279498</td>\n",
       "      <td>-3.141809</td>\n",
       "      <td>-0.651873</td>\n",
       "      <td>0.602982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129992</td>\n",
       "      <td>-0.241625</td>\n",
       "      <td>-0.170628</td>\n",
       "      <td>-0.072431</td>\n",
       "      <td>-0.563420</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.261329</td>\n",
       "      <td>4.510922</td>\n",
       "      <td>-7.080231</td>\n",
       "      <td>-0.277733</td>\n",
       "      <td>1.033016</td>\n",
       "      <td>1.857280</td>\n",
       "      <td>-0.347726</td>\n",
       "      <td>-3.141445</td>\n",
       "      <td>-0.651873</td>\n",
       "      <td>0.602982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129992</td>\n",
       "      <td>-0.241625</td>\n",
       "      <td>-0.122317</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>-1.050047</td>\n",
       "      <td>-0.995985</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.495656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4       PC5         5         8  \\\n",
       "0  0.060161  1.252939  0.644791 -0.418914  0.443708 -0.398089 -0.521063   \n",
       "1  0.340383  1.525171  0.900528 -1.114550  0.632546 -0.401396 -0.546879   \n",
       "2  4.967133  3.503331 -4.593643 -0.403800  0.503636  1.883791 -0.347726   \n",
       "3  7.268661  4.545243 -6.048977  0.167516 -1.561113 -0.072373 -0.279498   \n",
       "4  6.261329  4.510922 -7.080231 -0.277733  1.033016  1.857280 -0.347726   \n",
       "\n",
       "         38        51        67  ...        76        77        79        80  \\\n",
       "0 -3.142811  1.534041 -0.175252  ... -0.103169 -0.162696 -0.138420  0.030155   \n",
       "1 -3.142628  1.534041  1.575793  ... -0.049529 -0.320552 -0.218941 -0.226305   \n",
       "2 -3.142264 -0.651873  0.602982  ... -0.129992 -0.241625 -0.122317  0.030155   \n",
       "3 -3.141809 -0.651873  0.602982  ... -0.129992 -0.241625 -0.170628 -0.072431   \n",
       "4 -3.141445 -0.651873  0.602982  ... -0.129992 -0.241625 -0.122317  0.030155   \n",
       "\n",
       "         82       140       142       145       154  155  \n",
       "0  2.653966 -1.050047 -0.995985 -0.453058 -0.495656    0  \n",
       "1 -0.831461 -1.050047 -0.995985 -0.453058 -0.495656    0  \n",
       "2  0.109363 -1.050047 -0.995985 -0.453058 -0.495656    0  \n",
       "3 -0.563420 -1.050047 -0.995985 -0.453058 -0.495656    0  \n",
       "4  0.114777 -1.050047 -0.995985 -0.453058 -0.495656    0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_training = get_df(\"./train_imperson_without4n7_balanced_data.csv\")\n",
    "data_for_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' EXPLORING AND SELECTING ML ALGORITHMS (INDIVIDUAL 3) '''\n",
    "''' Timothy Chan '''\n",
    "\n",
    "'''Import Models and create the training sets'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = data_for_training.iloc[:,:-1]\n",
    "y = data_for_training.iloc[:,-1]\n",
    "\n",
    "columnnames = list(data_for_training)\n",
    "\n",
    "\n",
    "##Creates training and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) #75% training data, 25% test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create initial randomforest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12187     0]\n",
      " [    1 12073]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12187\n",
      "           1       1.00      1.00      1.00     12074\n",
      "\n",
      "    accuracy                           1.00     24261\n",
      "   macro avg       1.00      1.00      1.00     24261\n",
      "weighted avg       1.00      1.00      1.00     24261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create model with 1000 trees to start. This includes bagging. Set bootstrap = False to turn off\n",
    "\n",
    "randf = RandomForestClassifier(n_estimators = 100, random_state = 22)\n",
    "randf.fit(X_train, y_train)\n",
    "\n",
    "yhat = randf.predict(X_test)\n",
    "\n",
    "\n",
    "#Metrics and variable importance\n",
    "print(confusion_matrix(y_test,yhat))\n",
    "print(classification_report(y_test,yhat))\n",
    "\n",
    "##Nothing really stands out...We get very good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR RANDOM FOREST \n",
      "\n",
      "[[12184     3]\n",
      " [   15 12059]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12187\n",
      "           1       1.00      1.00      1.00     12074\n",
      "\n",
      "    accuracy                           1.00     24261\n",
      "   macro avg       1.00      1.00      1.00     24261\n",
      "weighted avg       1.00      1.00      1.00     24261\n",
      "\n",
      "RANDOMFOREST FEATURE IMPORTANCE \n",
      "\n",
      "[('67', 0.14108670846579247), ('76', 0.12987859781919708), ('79', 0.12472801619605255), ('38', 0.11166687453037899), ('PC2', 0.10446709162937193), ('5', 0.0704486614873539), ('71', 0.05238747329725532), ('75', 0.040449543516951465), ('80', 0.033611787312289296), ('77', 0.032454053795041016), ('82', 0.031260582218656754), ('PC3', 0.024976799610272153), ('8', 0.020503674624172435), ('PC1', 0.01578909535238207), ('142', 0.013152583771653707), ('73', 0.011891817726907143), ('51', 0.01071654997522805), ('PC4', 0.009958530425769184), ('140', 0.00991555072236264), ('PC5', 0.008475046166070184), ('154', 0.002178079159585988), ('145', 2.882197255649493e-06)]\n"
     ]
    }
   ],
   "source": [
    "##Let's limit the depth to 6\n",
    "\n",
    "\n",
    "randfs = RandomForestClassifier(n_estimators = 100, random_state = 22, max_depth=6 )\n",
    "\n",
    "randfs.fit(X_train, y_train)\n",
    "\n",
    "yhats = randfs.predict(X_test)\n",
    "\n",
    "##Results Metrics\n",
    "\n",
    "print(\"RESULTS FOR RANDOM FOREST \\n\")\n",
    "print(confusion_matrix(y_test,yhats))\n",
    "print(classification_report(y_test,yhats))\n",
    "LRimportance = [(feature,importance) for feature, importance in zip(columnnames,randfs.feature_importances_)]\n",
    "LRimportance.sort(key = lambda x:x[1], reverse=True) #returns a list of most important features\n",
    "\n",
    "print(\"RANDOMFOREST FEATURE IMPORTANCE \\n\")\n",
    "print(LRimportance)\n",
    "\n",
    "\n",
    "##19 misclassficiations very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Linear Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR LOGISTIC REGRESSION \n",
      "\n",
      "[[11980   207]\n",
      " [  114 11960]]\n",
      " LR Accuracy: 0.98676888833931\n",
      "LR Precision: 0.9829867674858223\n",
      "LR Recall: 0.9905582242835845\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAcaklEQVR4nO3de7idZXnn8e+PcBIUIpURg7ap08hQgtI2bVUwhEPbWIsoglqFwTotPXBVwWGsrV5TT9PRaTvVjrVKHVuKpS0yYCiVIAeTgNRAgghSRUFEtCkVUqIoEA73/PE+kUXcO1l77bX3Xsn6fq5rX/t9n/Ue7vUA++Y9PPeTqkKSpFGzy1wHIEnSRExQkqSRZIKSJI0kE5QkaSSZoCRJI2nXuTrx0572tFq4cOFcnV6SNCLWr19/T1Xtv3X7nCWohQsXsm7durk6vSRpRCS5c6J2b/FJkkaSCUqSNJJMUJKkkWSCkiSNJBOUJGkkmaAkSSPJBCVJGkkmKEnSSDJBSZJG0pxVkrj5m5tY+JZ/nKvTS5Km4WvvecmMn2O7V1BJliW5M8mqJCuS7JnkTUnWJLkmyft7tl2Q5MEkPzazYUuSdnb93uI7t6qWAdcCJwEHAUdW1RHAhT3bnQF8dqgRSpLG0lRv8d0IrAQWVVUBVNVqgCT7A08BvjbMACVJ42mqL0ksBTYDGyb47AzgA9vaOclpSdYlWffo9zZN8dSSpHHSb4I6JcmngfnAecCC3g+TzAeeVVW3bOsgVXV2VS2pqiXz9tp3oIAlSeOh31t851bV2wCSvBg4K8lvVFUlWQo8BCxKshI4FHgmcOyMRCxJGgtTfs28qi5NcjCwOskuwPqqeiPwAoAkfwW8e6hRSpLGTtq7DrNuyZIl5Yy6kqQk66tqydbtVpKQJI0kK0lImnWzUYVAO76BElSSvYCPA3sDm4BXAm8AjgfuBF5XVQ8PK0hJ0vgZ9BbfcmBtqy5xHfBq4KhWWeIm4GXDCU+SNK4GTVC3A3u05fnAQmBVW78CeP60opIkjb1BE9RXgJ9NcguwBLgN+Hb7bBPw1Il2spKEJKlfgyaoU4HLquoQ4B/pnmXt0z7bB7hvop2sJCFJ6tegCSrAxrZ8D90tviPb+rFY0VySNE2DvmZ+HvD3SU4BHgZeBfxakmuArwPvG1J8kqQxZSUJSdKcspKEJGmHYiUJzTirBkgaxMBXUEn+c5Irk6xKcnj7vSrJHUnOGGaQkqTxM2ipowOBI6vqmJ7mZe2zFcAl0w9NkjTOBr2C+gVgXruC+j9J5gEk2Rs4oKpuG1qEkqSxNGiCejqwe7uC+h5dkViAFwMrJ9vJShKSpH4NmqA2Aavb8lXAwW355cCFk+1kJQlJUr8GTVDXAs9ty4cBdyTZDTi4qj4/lMgkSWNtoJckqurGJA8kWUVX6uhPgKPprqYkSZo2K0lIkuaUlSQkSTsUK0lou6wEIWkubPcKKsmyJHe2KhErkuyZ5E1J1iS5Jsn723YfTPKtJL8682FLknZ2/V5BnVtVb0vyO8BJwEF0lSQqyZZ5oN4FXDeFY0qSNKmpJpMb6QbiLqr2dkVVrW6/NyTZ5s5JTgNOA5i3z/5TDlaSND6m+pLEUmAzsGGQkzlQV5LUr34T1ClJPg3Mp5tNd8HMhSRJUv8J6tyqOqqqTgfOB85Ku5+XZOmMRSdJGltTfqGhqi5NcjCwOskuwHpgTZK3Aq8BkmRBVb1zW8c59MB9Wefry5KkSVhJQpI0pyarJOFAXTkQV9JIstSRJGkk9VNJYkGSG5I8mGTXnvZXJLmrZ/21Sa5NckmSfWYqYEnSeOjnCmojcAzw2a3aTwTuAmhzQf0G3Tipc4FfH2KMkqQxtN0EVVUPVtW/97YleQlwOfBYa3oOcHNVPQJcATx/omM55bskqV+DPoM6FfhYz/p84NtteRPw1Il2spKEJKlfU05QSY4Grq2qzT3N9wFbnjvt09YlSRrYIFdQi4GXJlkJHJLk3cCXgcVJ5gHH8oPPqyRJmpLtjoNqL0BcCjwPuAz4var60/bZNVX1trb8F8DVwL/TVZTYJitJSJK2ZbsJqqoeprsqmuizI3qWz6V7g0+SpGmzksQYsWKEpB3JQAN1k2xqU8CvSrJfz7Y/maR6B/RKkjSIfhLJloG6F/W03VxVyybY9nTghiHEJUkacwMN1AUOTnJ1kvf0zAt1CF1lie/MQJySpDEz6EDdRXRljZ4KHNfazgQ+sK2drCQhSerXQAmqqjZWN5HUJ+jGPy0CNlXVPdvZz0oSkqS+DFJJYu82IBfgcOB24FDgp9vg3ecCHxpeiJKkcdTPW3y7JbmCxwfqLgauT3I18Czggqq6sKqWVtVy4Ca6yuaSJA3MKd8lSXNqsinfnVFXkjSSrCQx4qz+IGlcDXPK91t7qkv8+EwFLEkaD4NWkoCeKd+bb01SXUKSpCkb1pTvAPslWZPkw0n2HHKckqQxM6wp3wGOqKqlwJ3AaRPtZCUJSVK/hjXlO1W1sS1eRDdW6gdYSUKS1K+hTPmeZPcke7TPt1SXkCRpYEOZ8j3J04FLk9xPN+X7yTMYsyRpDFhJQpI0p6wkIUnaoVhJYoRZRULSOOunksSyJHe2ChErkuyZ5E1tzNM1Sd7ftvuHNsvulUmeOfOhS5J2Zv3e4ju3VYm4FjgJOAg4sqqOAC5s27yhql4EvIdudl1JkgY21Vt8NwIrgUVtRl2qanX7fUfb5hHg0aFFKEkaS1N9SWIpsBnYMNGHbabdtwIfnuRzK0lIkvrSb4I6JcmngfnAecCCSbb7Y+Cvq2rCgbpWkpAk9Wsqz6COqqrTgfOBs5IEIMnS9vu/AFVVfz0zoUqSxsmUx0FV1aXArcDqJNcAr2gffRBY0t72e8cQY5QkjSErSUiS5pSVJCRJOxQrSYwYq0dIUmdoCSrJXsDHgb2BTcArq+qhYR1fkjRehnmLbzmwtlWcuK6tS5I0kGEmqNuBLZMWzgfuHeKxJUljZpgJ6ivAzya5BVhCV7fvCawkIUnq1zAT1KnAZVV1CPCPTDCrrpUkJEn9GmaCCrCxLd8DmIEkSQMb2kDdJPOBv6d7DvUw8Kqq2jjZ9g7UlSTB5AN1h/aaeVXdB/zCsI4nSRpvDtQdEQ7QlaQnstSRJGkkDZSgkixMcnerXP6pJLsl+ack9yf5sWEHKUkaP9O5xXd5VZ0M0OaGehnw3qFEJUkae9O5xXdUkquTnFmdu7e3gwN1JUn9GjRBbQCeAxwFHJvkuf3s5EBdSVK/BkpQVfVQVX23qh4BLgEWDzcsSdK4G/Qliaf0rB5OVyhWkqShGfQliRcleRfwEHBNVa1Ncj5wBLAoyf+qqhXbOsChB+7LOsf+SJImMVCCqqpPAp/cqu2VQ4lIkiSsJDFlVnyQpNkx6DOo5W2Q7qokG5Kc5EBdSdIwDXqLbyWwEiDJ2ra8BgfqSpKGZFq3+JI8G7i7qr4DfKcrKCFJ0vRNt1jsCcBF/W5sJQlJUr+mm6COAy7ud2MrSUiS+jVwgkpyALC5qu4dYjySJAHTu4I6Hvj+YNw2UPfngXOSHD/dwCRJ4y1VNScnXrJkSa1bt25Ozi1JGh1J1lfVkq3bnVFXkjSSrCTRWCFCkkbLdq+gkixLcmerGrEiyZ5J3pRkTZJrkry/bXd+ktWt7aCZD12StDPr9xbfuVW1DLgWOAk4CDiyqo4ALmzbvLaqjgTeCrxh2IFKksbLVG/x3UhX1mhRtbcrqmp1+/1w2+bJwE1Di1CSNJammqCWApvppnx/giS7A1cBC4CXT7RzktOA0wDm7bP/FE8tSRon/d7iOyXJp4H5wHl0SegJqmpzu+V3EvDOiQ5iJQlJUr+m8gzqqKo6HTgfOCutMmySpens1rb9NvDADMQqSRojU37NvKouTXIwsDrJLsB64DpgZZICCjh9uGFKksaNlSQkSXPKShKSpB2KlSSwioQkjaKBrqCSLG+VJVYl2ZDkjJ71O5KcMexAJUnjZaArqKpaSTdglyRrgY9U1fva+grgkqFFKEkaS9N6BpXk2cDdVXV/W98bOKCqbhtGcJKk8TXdlyROAC7qWX8x7cpqIklOS7IuybpHv7dpmqeWJO3MppugjgMu7ll/OY8Xj/0BVpKQJPVr4ASV5ABgc1Xd29Z3Aw6uqs8PKzhJ0viazhXU8cCKnvWj6YrFSpI0bVaSkCTNKStJSJJ2KDt9JQmrREjSjmm7V1BJFiS5IcmDSXbtaX9Fkrva8lOSXJlkTZJLkjxlJoOWJO38+rnFtxE4BvjsVu0nAne15YeBk6tqKd2LE68bVoCSpPG03QRVVQ9W1b/3tiV5CXA58FjPNlumgX8EeHTYgUqSxsugL0mcCnxs68YkTwZOo5sW/gdYSUKS1K8pJ6gkRwPXVtXmrdoDfBR4a1XdN9G+VpKQJPVrkCuoxcBLk6wEDkny7tb+TuAzVeVgXUnStPXzFt9uSa4AngdcBqytqqOrajlwS1W9LckC4HeAl7c5oX5zZsOWJO3srCQhSZpTVpKQJO1QdqpKElaNkKSdx0BXUEmWt2dNq5JsSHJikmuTrE5ycZInDTtQSdJ4GShBVdXKqlpWVcuAr9MN2j2iqo4E1gO/NLwQJUnjaFq3+JI8G7i7qnpH3c4DvjLJ9qfRDeRl3j77T+fUkqSd3HRfkjgBuAggyc8kWUc3ceEdE23sQF1JUr+mm6COAy4GqKrr2muCFwGvn25gkqTxNnCCSnIAsLmq7k2ye89H3wYemHZkkqSxNp1nUMfTTa0BcFiSP6Srbr4ROGV7Ox964L6s87VwSdIkBk5QVfXhnuXrgCOHEpEkSeygA3UdkCtJOz9LHUmSRtJAV1BJlgNvaasHAb9JNy38ocBXgV+rKmfVlSQNbFiVJO4Fdm/rt2AlCUnSNE3rFt+WShLAAuCm1nwj8IJJtnfKd0lSX4ZVSeJWHn+L72jgqRNtbCUJSVK/hlJJoqpuBL6Q5NPAPnRXVZIkDWwolSQAquqdVXUU3fOo4U70JEkaO0OpJJFkF+Aq4FHgyqpau72drSQhSdqWYVWSeAxYNoyAJEkCK0lIkkbUdp9BJVmW5M42vfuKJHsmeVOSNUmuSfL+tt2qNuX7qiRHz3zokqSdWb9XUOdW1duS/A5wEl31iCOrqpL0Fok9pqoeGXqUkqSxM9VbfDcCK4FFVVUAVbW6ffYYcEWSfwV+q6o2Di9MSdK4mepr5kuBzcCGCT47sZU6uhh420Q7W0lCktSvfhPUKW0Q7nzgPLrSRk/Qc8V0EbB4ooNYSUKS1K9+E9S5VXVUVZ0OnA+clSQASZa23/u0bQ8Hbh96pJKksTLl18yr6tIkBwOr2wDd9cAa4KokDwAPAq8bapSSpLGT9q7DrFuyZEmtW7duTs4tSRodSdZX1ZKt251RV5I0kka6koQVIyRpfA065ftewMeBvYFNwJuBjwAFfAM4xSnfJUnTMegtvuXA2jbu6Trg+cBxVbUUuAP4xeGEJ0kaV4MmqNuBPdryfOD2qrqvrT9CN+2GJEkDGzRBfQX42SS3AEuAawGSLACOBT410U5WkpAk9WvQBHUqcFlVHUI3e+7JSfYAzgF+bbKCsVaSkCT1a9AEFWBLaaN7gH2Bs4EPVtU/DyMwSdJ4GzRBnQe8Mskq4LXADcAJwBvbfFAvH1J8kqQxZSUJSdKcspKEJGmHYiUJSdJIGugKKsny9qxpVZINSV6W5OeSXNXafmrYgUqSxstAV1BVtZJu6neSrAWuBP4S+DlLHEmShmFaz6CSPBu4G/hp4DHg0iTnJtl7GMFJksbXdF+SOIFuivenA88AXkxXVeLXJ9rYShKSpH5NN0EdB1xMV9H8mnZ77yrg4Ik2tpKEJKlfAyeoJAcAm6vqXuB6Hk9Kh9FVNJckaWDTec38eGAFQFV9K8nqJGuA7wGvGUZwkqTxZSUJSdKcspKEJGmHMnKVJKweIUmCIV5BJVmY5O5WSWLCCQslSerXsK+gLq+qk4d8TEnSGBr2M6ijklyd5MwhH1eSNGaGmaA2AM8BjgKOTfLcrTewkoQkqV9DS1BV9VBVfbeqHgEuARZPsI2VJCRJfRnmSxJP6Vk9HLh9WMeWJI2fYb4k8aIk7wIeoqvLt3ZbGx964L6s85VySdIkhpagquqTwCeHdTxJ0nibs0oSN3/TlyQkSZOz1JEkaSQNdIsvyWLgbOBR4DbgV4FzgGcBDwOvrqp7hhWkJGn8DHoFdWtVvbCqXtTWl9DNDXUk8JfAa4cSnSRpbA2UoKrq4Z7Vh4B/BbbM2zEfuHei/RyoK0nq18Bv8SV5KfAHwJfpEtQeSb5Id9vvZybap6rOprs1yB7PWDQ3E1FJknYIA78kUVUXV9Vi4JvACcCmqjoYeDtw1nDCkySNq4ESVJI9ela/DWwCNrb1ewDrGEmSpmXQW3zLk7ypLX8FuAJ4fZJVdEnvV7Z3gEMPNIdJkiY3UIKqqhXAiq2aT5x+OJIkdawkIUkaSYM+g1repnZflWRDkpcl2dTTtt+wA5UkjZdBb/GtBFYCJFlL9wzq5qpaNrzQJEnjbFq3+JI8G7i7qu4HDm7Tvb8nSYYTniRpXE33GdQJwEVteRGwFHgqcNxEG1tJQpLUr+kmqOOAiwGqamNVFfAJJpjuvW3jlO+SpL4MnKCSHEBXIPbeJHsnmdc+crp3SdK0TecK6ngeHwu1CLg+ydV0U25cMN3AJEnjbeBisVX14Z7lG4GfnMr+VpKQJG2LM+pKkkaSCUqSNJJMUJKkkWSCkiSNJBOUJGkkmaAkSSPJBCVJGkkmKEnSSDJBSZJGkglKkjSS0hUgn4MTJ98Bbp2Tk+9YngbcM9dBjDj7qD/2U3/sp/4Ms59+pKr237px4Fp8Q3BrVS2Zw/PvEJKss5+2zT7qj/3UH/upP7PRT97ikySNJBOUJGkkzWWCOnsOz70jsZ+2zz7qj/3UH/upPzPeT3P2koQkSdviLT5J0kgyQUmSRtKsJ6gky5PcmuS2JG+Z7fOPqiTPSvLpJF9MckuSN7b2/ZJcnuQr7fdT5zrWUZBkXpLPJbmkrdtPW0kyP8kFSb7U/r16gf30REnObP+9fSHJ3ybZ0z7qJPlokn9L8oWetkn7Jsnvtr/rtyb5hWHEMKsJKsk84M+AFwM/Dvxykh+fzRhG2CPAf62qg4HnA6e3vnkLcGVVLQKubOuCNwJf7Fm3n37Q+4GVVfWfgOfR9Zf91CQ5EHgDsKSqFgPzgFdjH23xV8Dyrdom7Jv2t+rVwCFtnw+2v/fTMttXUD8D3FZVX62qzcDfAcfPcgwjqao2VNUNbfk7dH9MDqTrn3PaZucAL5ubCEdHkmcCLwE+0tNsP/VIsg+wFPi/AFW1uaruw37a2q7Ak5LsCuwF/Av2EQBVtQbYuFXzZH1zPPB3VfVQVd0B3Eb3935aZjtBHQjc1bP+jdamHkkWAj8BrAWeXlUboEtiwH+Yu8hGxvuANwOP9bTZT0/0bOBbwF+2W6EfSbI39tP3VdU3gT8Cvg5sADZV1aewj7Zlsr6Zkb/ts52gMkGb77n3SPJk4P8BZ1TVt+c6nlGT5JeAf6uq9XMdy4jbFfhJ4M+r6ieA7zK+t6om1J6fHA/8KLAA2DvJyXMb1Q5rRv62z3aC+gbwrJ71Z9JdUgtIshtdcvqbqrqwNd+d5Bnt82cA/zZX8Y2Iw4GXJvka3S3io5N8DPtpa98AvlFVa9v6BXQJy3563LHAHVX1rap6GLgQeCH20bZM1jcz8rd9thPU9cCiJD+aZHe6h2oXz3IMIylJ6J4XfLGq/nfPRxcDp7blU4EVsx3bKKmq362qZ1bVQrp/f66qqpOxn56gqv4VuCvJQa3pGOCfsZ96fR14fpK92n9/x9A9+7WPJjdZ31wMvDrJHkl+FFgEXDfdk816JYkkv0j3DGEe8NGq+h+zGsCISnIEcDVwM48/W/k9uudQ5wM/TPcf1ElVtfWDy7GUZBlwVlX9UpIfwn56giSH0b1IsjvwVeBX6P6n1H5qkrwDeBXdW7SfA34VeDL2EUn+FlhGN63G3cDvA59gkr5J8lbg9XR9eUZVXTrtGCx1JEkaRVaSkCSNJBOUJGkkmaAkSSPJBCVJGkkmKEnSSDJBacYleTTJja1i9D8kmd/HPvdv5/P5SX6rZ31BkguGEOvCJA+0eLf87D7gcV4z3Xi2cfy3Jzlrpo4/yTlfl2TBbJ5T480EpdnwQFUd1ipGbwROH8Ix5wPfT1BV9S9VdeIQjgtwe4t3y8/mAY6xEJhyghpGBeiZ0OJ6HV1JIGlWmKA02/6JniKSSf5bkuuT3NQGTT5BkicnuTLJDUluTrKl+v17gP/YrnD+sF2xfKHtszbJIT3HWJXkp5Ls3ea4ub4VUO27kv5k+7bzXt3iuyHJC3vie1GL78x29fGBnuNd0gYak+T+JO9MshZ4QYt1dZL1SS7bUlpmG7GtSvInSdakm/fpp5NcmG7Onnf3xPmlJOe0vr4gyV7ts2Pad7q5fcc9WvvXkvz3JNcAvwwsAf6mfacntc+ub1fGZ7dqDFvieW+S65J8OcmLWvu8JH/UznNTkt9u7VP6vhojVeWPPzP6A9zffs8DPg4sb+s/D5xNV2hyF+ASYOlW++wK7NOWn0ZXxj90Vyhf6DnH99eBM4F3tOVnAF9uy38AnNyW5wNfBvbeKtaFwAPAje3nz7a1L90UDXu29kXAura8DLik57ivAz7Qs34JsKwtF/DKtrwbcC2wf1t/FV3Fla379O10VTQAVgHvbctvpKuB9gxgD7oaaT/UvlcBh7ftPgqcBexJV4X6Oa39r+mqAAB8DXhzzzlX0c2dtGV9v57lc4Hjerb747b8i8AVbfk36WpN7rpl/36/rz/j+bMr0sx7UpIb6f5Irgcub+0/334+19afTPdHfk3PvgH+IMlSuhJQBwJP3875zm/n+H3glXRJccv5Xtrz7GZPupItX9xq/9ur6rCt2ibb91+AD7SyQo8Cz9lObBN5lO4PN8BBwGLg8nZBMo9uKojt2VLT8mbglmpTIiT5Kl0Rz/uAu6rqM227j9FN1nc5XcHUL7f2c+huwb6vrf/9Ns55VJI30yXp/YBbgH9on20pdrye7p87dMVZP1RVjwBU1cYkiwf8vhoDJijNhgeq6rAk+9JdOZwO/Cld8vmfVfXhbez7WmB/4Keq6uF0Vcz33NbJquqbSe5N8ly6/yP/9fZRgFdU1a0DfIcJ903ydro6Zc+juwp8cJL9H+GJt9R7v8ODVfVoz3luqaoXTDG+h9rvx3qWt6xv+e9867pmxcTTJPT67kSNSfYEPkh3RXVX64fe77Qlhkd7zp8JYhj0+2oM+AxKs6aqNtH9X/tZ6aYWuQx4fbo5sEhyYJKtJ4fbl27+p4eTHAX8SGv/DvCUbZzu7+gmNdy3qm5ubZcBv93zrOQnphD+ZPvuC2yoqseAU+iuACaK72vAYUl2SfIsJp9t9FZg/yQvaOfZrfd52jT98Jbj0j1Tugb4ErAwyY+19lOA1ZPs3/udtiSje9o/v35eUPkU8BvpZq8lyX7M7PfVDs4EpVlVVZ8DPg+8urrZS88D/inJzXRzFm2ddP4GWJJkHd3V1Jface4FPtMe0P/hBKe6gG46jvN72t5F98zjpvZCxbumEPpk+34QODXJZ+lu72254rgJeCTJ55OcCXwGuIPuFtwfATdMdJLq3hg8EXhvks/TPQd74UTbDuCLLdab6G7J/XlVPUhX5fzj7Z/BY8CHJtn/r4APtdu1DwF/0b7PJ+im0tmej9BVwL6pfbfXzPD31Q7OaubSGEiykO6ljcVzHIrUN6+gJEkjySsoSdJI8gpKkjSSTFCSpJFkgpIkjSQTlCRpJJmgJEkj6f8DOs7niVSR1+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=0)\n",
    "\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "yhat= logreg.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, yhat)\n",
    "\n",
    "print(\"RESULTS FOR LOGISTIC REGRESSION \\n\")\n",
    "\n",
    "print(cnf_matrix)\n",
    "\n",
    "\n",
    "print(\" LR Accuracy:\",metrics.accuracy_score(y_test, yhat))\n",
    "print(\"LR Precision:\",metrics.precision_score(y_test, yhat))\n",
    "print(\"LR Recall:\",metrics.recall_score(y_test, yhat))\n",
    "\n",
    "\n",
    "feature_importance = abs(logreg.coef_[0])\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(X.columns)[sorted_idx], fontsize=8)\n",
    "featax.set_xlabel('Relative Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##Large number of misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "##Create an ROC curve. ROC score is high 0.99. Seems classifer is good.\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create K-NN model. With Euclidean distance and 5 neighbours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR KNN K=5 \n",
      "\n",
      "[[12165    22]\n",
      " [    5 12069]]\n",
      " KNN #3 Accuracy: 0.998887102757512\n",
      "KNN #3 Precision: 0.9981804648085353\n",
      "KNN #3 Recall: 0.9995858870299817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Fit\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "#Predict\n",
    "\n",
    "knnpredict = knn.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, knnpredict)\n",
    "\n",
    "\n",
    "print(\"RESULTS FOR KNN K=5 \\n\")\n",
    "print(cnf_matrix)\n",
    "\n",
    "\n",
    "print(\" KNN #3 Accuracy:\",metrics.accuracy_score(y_test, knnpredict))\n",
    "print(\"KNN #3 Precision:\",metrics.precision_score(y_test, knnpredict))\n",
    "print(\"KNN #3 Recall:\",metrics.recall_score(y_test, knnpredict))\n",
    "\n",
    "##Very good accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with 2 layers with 5 Neurons. 1000 iterations of backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12167    20]\n",
      " [    3 12071]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12187\n",
      "           1       1.00      1.00      1.00     12074\n",
      "\n",
      "    accuracy                           1.00     24261\n",
      "   macro avg       1.00      1.00      1.00     24261\n",
      "weighted avg       1.00      1.00      1.00     24261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5, 5), max_iter=1000) #two layers of 5 neurons, 1000 of backprop\n",
    "mlp.fit(X_train, y_train.values.ravel()) #train the algo\n",
    "\n",
    "predictions = mlp.predict(X_test) #make predictions on the xtest set\n",
    "\n",
    "print(confusion_matrix(y_test,predictions)) #results are good\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with RBF kernel, gamma = 1/n, and relatively low Cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12165    22]\n",
      " [   10 12064]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12187\n",
      "           1       1.00      1.00      1.00     12074\n",
      "\n",
      "    accuracy                           1.00     24261\n",
      "   macro avg       1.00      1.00      1.00     24261\n",
      "weighted avg       1.00      1.00      1.00     24261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='rbf',gamma='auto',C=1)\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' REFINING ALGORITHMS (INDIVIDUAL 4) '''\n",
    "''' Cosmin Stanciu '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' EVALUATING MODEL AND ANALYZING THE RESULTS (INDIVIDUAL 5) '''\n",
    "''' Mike Jun Ming'''\n",
    "#save model using pickle\n",
    "from pickle import dump\n",
    "\n",
    "#save the model\n",
    "modelname = 'randf.sav'\n",
    "modelname1 = 'randfs.sav'\n",
    "modelname2 = 'logreg.sav'\n",
    "modelname3 = 'knn.sav'\n",
    "modelname4 = 'mlp.sav'\n",
    "modelname5 = 'svclassifier.sav'\n",
    "\n",
    "dump(randf, open(modelname,'wb'))\n",
    "dump(randfs, open(modelname1,'wb'))\n",
    "dump(logreg, open(modelname2,'wb'))\n",
    "dump(knn, open(modelname3,'wb'))\n",
    "dump(mlp, open(modelname4,'wb'))\n",
    "dump(svclassifier, open(modelname5,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>14</th>\n",
       "      <th>38</th>\n",
       "      <th>48</th>\n",
       "      <th>51</th>\n",
       "      <th>61</th>\n",
       "      <th>67</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>...</th>\n",
       "      <th>125</th>\n",
       "      <th>130</th>\n",
       "      <th>138</th>\n",
       "      <th>140</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88219</td>\n",
       "      <td>0.98108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69231</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22215</td>\n",
       "      <td>0.47729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>0.98108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69231</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.35106</td>\n",
       "      <td>0.18516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>0.98108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67949</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.43700</td>\n",
       "      <td>0.47541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>0.98108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>0.98108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69231</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63621</td>\n",
       "      <td>0.89971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          5        8  14       38       48  51       61       67  70  71  ...  \\\n",
       "0  0.002547  0.98824   1  0.88219  0.98108   1  0.69231  0.61538   0   0  ...   \n",
       "1  0.003296  0.98824   1  0.88220  0.98108   1  0.69231  0.61538   0   0  ...   \n",
       "2  0.003285  0.98824   1  0.88220  0.98108   1  0.67949  0.61538   0   0  ...   \n",
       "3  0.005942  0.00000   1  0.88220  0.98108   1  0.75641  1.00000   0   0  ...   \n",
       "4  0.001519  0.98824   1  0.88220  0.98108   1  0.69231  0.61538   1   0  ...   \n",
       "\n",
       "   125  130  138      140      142  143  144      145      154  155  \n",
       "0  0.0    0    0  0.22215  0.47729  0.0  0.0  0.16667  0.98674    0  \n",
       "1  0.0    0    0  0.35106  0.18516  0.0  0.0  0.16667  0.98674    0  \n",
       "2  0.0    0    0  0.43700  0.47541  0.0  0.0  0.16667  0.98674    0  \n",
       "3  0.0    0    0  0.00000  0.00000  0.0  0.0  0.00000  0.00000    0  \n",
       "4  0.0    0    0  0.63621  0.89971  0.0  0.0  0.16667  0.98674    0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "testfile = pd.read_csv(\"test_imperson_without4n7_balanced_data.csv\")\n",
    "\n",
    "processedtest_df = testfile.loc[:,['5','8','14','38','48','51','61','67','70','71','72','73','75','76','77',\n",
    " '78','79','80','82','83','88','93','94','104','105','106','109','110',\n",
    " '111','112','113','117','120','121','122','123','125','130','138','140',\n",
    " '142','143','144','145','154','155']]\n",
    "\n",
    "processedtest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>38</th>\n",
       "      <th>51</th>\n",
       "      <th>67</th>\n",
       "      <th>71</th>\n",
       "      <th>73</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>82</th>\n",
       "      <th>140</th>\n",
       "      <th>142</th>\n",
       "      <th>145</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88219</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.57753</td>\n",
       "      <td>0.22215</td>\n",
       "      <td>0.47729</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.58559</td>\n",
       "      <td>0.35106</td>\n",
       "      <td>0.18516</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.59096</td>\n",
       "      <td>0.43700</td>\n",
       "      <td>0.47541</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.60342</td>\n",
       "      <td>0.63621</td>\n",
       "      <td>0.89971</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          5        8       38  51       67  71  73        75        76  \\\n",
       "0  0.002547  0.98824  0.88219   1  0.61538   0   1  0.003034  0.005291   \n",
       "1  0.003296  0.98824  0.88220   1  0.61538   0   1  0.003034  0.005291   \n",
       "2  0.003285  0.98824  0.88220   1  0.61538   0   1  0.003034  0.005291   \n",
       "3  0.005942  0.00000  0.88220   1  1.00000   0   0  0.000000  0.015873   \n",
       "4  0.001519  0.98824  0.88220   1  0.61538   0   1  0.003034  0.005291   \n",
       "\n",
       "         77        79       80       82      140      142      145      154  \\\n",
       "0  0.000343  0.000128  0.00241  0.57753  0.22215  0.47729  0.16667  0.98674   \n",
       "1  0.000343  0.000128  0.00241  0.58559  0.35106  0.18516  0.16667  0.98674   \n",
       "2  0.000343  0.000128  0.00241  0.59096  0.43700  0.47541  0.16667  0.98674   \n",
       "3  0.000000  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "4  0.000343  0.000128  0.00241  0.60342  0.63621  0.89971  0.16667  0.98674   \n",
       "\n",
       "   155  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedtestfs_df = testfile.loc[:,['5','8','38','51','67','71','73','75','76','77',\n",
    " '79','80','82','140','142','145','154','155']]\n",
    "\n",
    "processedtestfs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>38</th>\n",
       "      <th>51</th>\n",
       "      <th>67</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>82</th>\n",
       "      <th>140</th>\n",
       "      <th>142</th>\n",
       "      <th>145</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.302851</td>\n",
       "      <td>-0.459643</td>\n",
       "      <td>0.186081</td>\n",
       "      <td>-0.120477</td>\n",
       "      <td>-0.112876</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88219</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.57753</td>\n",
       "      <td>0.22215</td>\n",
       "      <td>0.47729</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.285858</td>\n",
       "      <td>-0.412334</td>\n",
       "      <td>0.183265</td>\n",
       "      <td>-0.358312</td>\n",
       "      <td>-0.036705</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.58559</td>\n",
       "      <td>0.35106</td>\n",
       "      <td>0.18516</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.352800</td>\n",
       "      <td>-0.488101</td>\n",
       "      <td>0.208511</td>\n",
       "      <td>-0.080741</td>\n",
       "      <td>-0.064376</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.59096</td>\n",
       "      <td>0.43700</td>\n",
       "      <td>0.47541</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.464336</td>\n",
       "      <td>1.066533</td>\n",
       "      <td>-0.585008</td>\n",
       "      <td>-0.018939</td>\n",
       "      <td>-0.018399</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.528352</td>\n",
       "      <td>-0.632822</td>\n",
       "      <td>0.317854</td>\n",
       "      <td>0.282630</td>\n",
       "      <td>-0.055167</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.98824</td>\n",
       "      <td>0.88220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.60342</td>\n",
       "      <td>0.63621</td>\n",
       "      <td>0.89971</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.98674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4       PC5         5        8  \\\n",
       "0  1.302851 -0.459643  0.186081 -0.120477 -0.112876  0.002547  0.98824   \n",
       "1  1.285858 -0.412334  0.183265 -0.358312 -0.036705  0.003296  0.98824   \n",
       "2  1.352800 -0.488101  0.208511 -0.080741 -0.064376  0.003285  0.98824   \n",
       "3  0.464336  1.066533 -0.585008 -0.018939 -0.018399  0.005942  0.00000   \n",
       "4  1.528352 -0.632822  0.317854  0.282630 -0.055167  0.001519  0.98824   \n",
       "\n",
       "        38  51       67  ...        76        77        79       80       82  \\\n",
       "0  0.88219   1  0.61538  ...  0.005291  0.000343  0.000128  0.00241  0.57753   \n",
       "1  0.88220   1  0.61538  ...  0.005291  0.000343  0.000128  0.00241  0.58559   \n",
       "2  0.88220   1  0.61538  ...  0.005291  0.000343  0.000128  0.00241  0.59096   \n",
       "3  0.88220   1  1.00000  ...  0.015873  0.000000  0.000000  0.00000  0.00000   \n",
       "4  0.88220   1  0.61538  ...  0.005291  0.000343  0.000128  0.00241  0.60342   \n",
       "\n",
       "       140      142      145      154  155  \n",
       "0  0.22215  0.47729  0.16667  0.98674    0  \n",
       "1  0.35106  0.18516  0.16667  0.98674    0  \n",
       "2  0.43700  0.47541  0.16667  0.98674    0  \n",
       "3  0.00000  0.00000  0.00000  0.00000    0  \n",
       "4  0.63621  0.89971  0.16667  0.98674    0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "pcaT = pca2.fit_transform(processedtest_df.iloc[:,:-1])\n",
    "\n",
    "processedtest_with_pcs = pd.DataFrame(pcaT,columns=[f\"PC{i}\" for i in range(1,6)]).join(processedtestfs_df)\n",
    "processedtest_with_pcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = processedtest_with_pcs.iloc[:,:-1]\n",
    "yt = processedtest_with_pcs.iloc[:,-1]\n",
    "\n",
    "columnnames = list(processedtest_with_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "modelname = 'mlp.sav'\n",
    "loaded_model = load(open(modelname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     20079\n",
      "           1       0.00      0.00      0.00     20079\n",
      "\n",
      "    accuracy                           0.50     40158\n",
      "   macro avg       0.25      0.50      0.33     40158\n",
      "weighted avg       0.25      0.50      0.33     40158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.score(Xt, yt) \n",
    "print(result)\n",
    "\n",
    "yrestmlp = mlp.predict(Xt)\n",
    "\n",
    "#Metrics and variable importance\n",
    "#print(confusion_matrix(yt,yrestmlp))\n",
    "print(classification_report(yt,yrestmlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[[20079     0]\n",
      " [20079     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     20079\n",
      "           1       0.00      0.00      0.00     20079\n",
      "\n",
      "    accuracy                           0.50     40158\n",
      "   macro avg       0.25      0.50      0.33     40158\n",
      "weighted avg       0.25      0.50      0.33     40158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "modelname2 = 'randfs.sav'\n",
    "loaded_modelrf = load(open(modelname2, 'rb'))\n",
    "\n",
    "result = loaded_modelrf.score(Xt, yt) \n",
    "print(result)\n",
    "\n",
    "yrest2 = loaded_modelrf.predict(Xt)\n",
    "\n",
    "#Metrics and variable importance\n",
    "print(confusion_matrix(yt,yrest2))\n",
    "print(classification_report(yt,yrest2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999003934458887\n"
     ]
    }
   ],
   "source": [
    "modelname3 = 'knn.sav'\n",
    "loaded_modelknn = load(open(modelname3, 'rb'))\n",
    "\n",
    "result = loaded_modelknn.score(Xt, yt) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999252950844166\n"
     ]
    }
   ],
   "source": [
    "modelname4 = 'logreg.sav'\n",
    "loaded_modellogreg = load(open(modelname4, 'rb'))\n",
    "\n",
    "result = loaded_modellogreg.score(Xt, yt) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "modelname5 = 'svclassifier.sav'\n",
    "loaded_modelsvc = load(open(modelname5, 'rb'))\n",
    "\n",
    "result = loaded_modelsvc.score(Xt, yt) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
